{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "\n",
    "Xbf,y = datasets.load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 569 and the array at index 1 has size 400",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Python\\machinelearningtest\\logistic_reg.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/machinelearningtest/logistic_reg.ipynb#ch0000003?line=6'>7</a>\u001b[0m y_train_new  \u001b[39m=\u001b[39m (y_train \u001b[39m-\u001b[39m y_train\u001b[39m.\u001b[39mmin()) \u001b[39m/\u001b[39m (y_train\u001b[39m.\u001b[39mmax()\u001b[39m-\u001b[39my_train\u001b[39m.\u001b[39mmin())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/machinelearningtest/logistic_reg.ipynb#ch0000003?line=7'>8</a>\u001b[0m one \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((Xbf\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Python/machinelearningtest/logistic_reg.ipynb#ch0000003?line=8'>9</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((one, X_train_new), axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 569 and the array at index 1 has size 400"
     ]
    }
   ],
   "source": [
    "X_new = np.array(Xbf)\n",
    "X_train,X_test = Xbf[:400],Xbf[400:]\n",
    "y_train,y_test = y[:400],y[400:]\n",
    "X_test_new1 = (X_test-X_train.min())/(X_train.max()-X_train.min())\n",
    "X_train_new = (X_train-X_train.min())/(X_train.max()-X_train.min())\n",
    "one = np.ones((X_train_new.shape[0],1))\n",
    "X = np.concatenate((one, X_train_new), axis = 1)\n",
    "one1 = np.ones((X_test_new1.shape[0],1))\n",
    "X_test_new1 = np.concatenate((one1,X_test_new1),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.22896098e-03, 2.44005642e-03, 2.88669488e-02, ...,\n",
       "        6.23883404e-05, 1.08157029e-04, 2.79501646e-05],\n",
       "       [4.83544899e-03, 4.17724495e-03, 3.12411848e-02, ...,\n",
       "        4.37235543e-05, 6.46450400e-05, 2.09261871e-05],\n",
       "       [4.62858486e-03, 4.99529854e-03, 3.05594734e-02, ...,\n",
       "        5.71227080e-05, 8.49318289e-05, 2.05876822e-05],\n",
       "       ...,\n",
       "       [3.90220969e-03, 6.60084626e-03, 2.54583921e-02, ...,\n",
       "        3.33333333e-05, 5.21391631e-05, 1.83826986e-05],\n",
       "       [4.84250118e-03, 6.89468735e-03, 3.29337094e-02, ...,\n",
       "        6.22943112e-05, 9.60742830e-05, 2.91490362e-05],\n",
       "       [1.82416549e-03, 5.76868829e-03, 1.12646921e-02, ...,\n",
       "        0.00000000e+00, 6.74894217e-05, 1.65467795e-05]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def logistic_function(self, t):\n",
    "        return 1/(1+np.exp(-t))\n",
    "    \n",
    "    def cost_function(self):                 \n",
    "        z= self.logistic_function(np.dot(self.X,self.beta.T))\n",
    "        return -np.mean(self.y*np.log(z)+(1-self.y)*np.log(1-z))\n",
    "    def grad_function(self):\n",
    "        N = self.X.shape[0]\n",
    "        z= self.logistic_function(np.dot(self.X,self.beta.T))\n",
    "        return 1/N*np.dot((z-self.y).T,self.X)\n",
    "        '''\n",
    "    def fit(self, X, y, tau=5, lr=0.02):        \n",
    "        loss = []\n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "                 \n",
    "        while self.cost_function(X,y,self.beta) >= tau:  \n",
    "            loss.append(self.cost_function(X,y,self.beta))      \n",
    "            self.beta -= lr*self.grad_function(X,y,self.beta)\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.self.beta = self.beta\n",
    "        self.loss = loss\n",
    "        return self.beta\n",
    "    '''\n",
    "    def fit(self,iteration = 100000,lr = 0.1):\n",
    "        self.beta = np.random.rand(self.X.shape[1])\n",
    "                 \n",
    "        for i in range(iteration):     \n",
    "            self.beta -= lr*self.grad_function()\n",
    "            if i % 1000 == 0:\n",
    "                print(self.cost_function())\n",
    "        self.beta = self.beta\n",
    "        return self.beta\n",
    "    def predict(self,X_test,y_test):        \n",
    "        k = self.logistic_function(np.dot(X_test,self.beta.T))\n",
    "        return np.array([1 if i >= 0.5 else 0 for i in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7106727196535333\n",
      "0.4952322328770266\n",
      "0.4145500989985404\n",
      "0.37105264613145983\n",
      "0.34355729388192213\n",
      "0.32445398815750404\n",
      "0.310326972317314\n",
      "0.29940875903845665\n",
      "0.2906894953661603\n",
      "0.2835483366916654\n",
      "0.27758142653370643\n",
      "0.27251401185443225\n",
      "0.26815223321477266\n",
      "0.26435511489382846\n",
      "0.26101750061448953\n",
      "0.25805923498489\n",
      "0.25541806780205584\n",
      "0.2530448618717417\n",
      "0.25090027296273887\n",
      "0.2489523975495073\n",
      "0.24717507284102475\n",
      "0.2455466262914196\n",
      "0.24404894103596936\n",
      "0.2426667473698937\n",
      "0.24138707858463024\n",
      "0.24019884807064493\n",
      "0.2390925170954651\n",
      "0.23805983121670687\n",
      "0.23709360923431333\n",
      "0.2361875727799692\n",
      "0.2353362076408132\n",
      "0.23453465008654872\n",
      "0.233778593060463\n",
      "0.23306420827360383\n",
      "0.23238808112333817\n",
      "0.23174715602370038\n",
      "0.23113869024262673\n",
      "0.23056021473128482\n",
      "0.23000950073284723\n",
      "0.22948453119378456\n",
      "0.22898347618595286\n",
      "0.22850467169421143\n",
      "0.22804660124086004\n",
      "0.2276078799114797\n",
      "0.22718724042186567\n",
      "0.22678352092652382\n",
      "0.22639565431863676\n",
      "0.22602265881181688\n",
      "0.22566362962714587\n",
      "0.22531773163636118\n",
      "0.224984192834719\n",
      "0.22466229853590805\n",
      "0.22435138619712416\n",
      "0.22405084079559426\n",
      "0.22376009068892858\n",
      "0.22347860390102703\n",
      "0.2232058847831861\n",
      "0.22294147100677356\n",
      "0.22268493084956384\n",
      "0.22243586074272026\n",
      "0.22219388304959772\n",
      "0.2219586440511406\n",
      "0.22172981211574838\n",
      "0.22150707603416142\n",
      "0.22129014350222945\n",
      "0.22107873973644182\n",
      "0.22087260620884094\n",
      "0.220671499489465\n",
      "0.22047519018579703\n",
      "0.22028346196985873\n",
      "0.22009611068461107\n",
      "0.21991294352221735\n",
      "0.21973377826751725\n",
      "0.2195584426007534\n",
      "0.21938677345421023\n",
      "0.21921861641796517\n",
      "0.2190538251904397\n",
      "0.21889226106986334\n",
      "0.21873379248314345\n",
      "0.2185782945489781\n",
      "0.2184256486723471\n",
      "0.2182757421677894\n",
      "0.21812846790911683\n",
      "0.21798372400343174\n",
      "0.21784141348750763\n",
      "0.2177014440447721\n",
      "0.21756372774128235\n",
      "0.21742818077923087\n",
      "0.21729472326664256\n",
      "0.2171632790020419\n",
      "0.21703377527297288\n",
      "0.21690614266734692\n",
      "0.216780314896683\n",
      "0.21665622863037748\n",
      "0.21653382334021495\n",
      "0.21641304115439358\n",
      "0.21629382672039696\n",
      "0.2161761270760966\n",
      "0.21605989152852134\n",
      "0.2159450715397666\n",
      "[  8.34653317   0.74261976  -0.04688522  -0.384995    -9.67671666\n",
      "   0.3561422    0.94143288   0.04041911   0.72749945   0.73678993\n",
      "   0.76386138   0.91755376   0.25503062   0.39992029  -1.49745095\n",
      "   0.16540561   0.44807593   0.24672135   0.18350507   0.1044125\n",
      "   0.1103993   -0.09859456  -0.59632806  -2.06358687 -33.06541386\n",
      "   0.33770317   0.46623175   0.23470081   0.08133192   0.12023086\n",
      "   0.0617165 ]\n"
     ]
    }
   ],
   "source": [
    "result = LogisticRegression(X_train_new,y)\n",
    "print(result.fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = abs(result.predict()-y_test_new)\n",
    "np.sum(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "682500ecd35da165753406cb9a5b7a11d766438723cad81a60a58ef1c8c50a88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
